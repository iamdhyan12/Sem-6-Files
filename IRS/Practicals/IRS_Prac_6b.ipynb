{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46f8d97f",
   "metadata": {},
   "source": [
    " <h1>20BCE020 <br>\n",
    "Practical-6 IRS <br>\n",
    "    Implement a program for document classification using Naive Bayes and Bernoulli. Compare the performance of both algorithms using suitable accuracy parameters. <br>\n",
    "Burnoulli Bayes </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb3337b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# nltk.download(\"all\")\n",
    "from nltk.stem import PorterStemmer\n",
    "stm=PorterStemmer()\n",
    "n=4\n",
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0514f7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Chinese', 'Beijing', 'Chinese'], ['Chinese', 'Chinese', 'Shanghai'], ['Chinese', 'Macao'], ['Tokyo', 'Japan', 'Chinese']]\n"
     ]
    }
   ],
   "source": [
    "fl=[]\n",
    "for i in range(1,n+1):\n",
    "    with open(\"N\"+str(i)+\".txt\",'r') as mf:\n",
    "        s=mf.read()\n",
    "        s=word_tokenize(s)\n",
    "        s=[w for w in s if w.casefold() not in stop_words]\n",
    "        ll=[]\n",
    "        for a in s:\n",
    "            if a==\"'\" or a==\".\" or a==',' or a==\":\":\n",
    "                continue\n",
    "            else:\n",
    "                ll.append(a)\n",
    "        fl.append(ll)\n",
    "print(fl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8243e980",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(fl)):\n",
    "    a=\"\"\n",
    "    for j in fl[i]:        \n",
    "        a+=j+\" \"\n",
    "    fl[i]=a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41259cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Chinese Beijing Chinese ', 'Chinese Chinese Shanghai ', 'Chinese Macao ', 'Tokyo Japan Chinese ']\n"
     ]
    }
   ],
   "source": [
    "print(fl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e22534b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ll=['c','c','c','j']\n",
    "unique=[]\n",
    "classes=[]\n",
    "class_p={}\n",
    "class_prior={}\n",
    "word_count={}\n",
    "\n",
    "word_prob={}\n",
    "for i in range(len(ll)):\n",
    "    if ll[i] not in classes:\n",
    "        classes.append(ll[i])\n",
    "        class_p[ll[i]]=1\n",
    "        word_count[ll[i]]={}\n",
    "    else:\n",
    "        class_p[ll[i]] += 1\n",
    "        \n",
    "for i in range(len(fl)):\n",
    "    words=fl[i].split()\n",
    "    for w in words:\n",
    "        if w not in unique:\n",
    "            unique.append(w)\n",
    "\n",
    "for j in unique:\n",
    "    for i in range(len(fl)):\n",
    "        word_count.setdefault(ll[i], {}).setdefault(j, 0)\n",
    "        if j in fl[i]:\n",
    "            word_count[ll[i]][j] += 1\n",
    "            \n",
    "\n",
    "for c in classes:\n",
    "    word_prob[c]={}\n",
    "    for w in unique:\n",
    "        word_prob[c][w]=(word_count[c][w]+1)/(class_p[c]+2)\n",
    "        \n",
    "total_docs=len(ll)\n",
    "for c in classes:\n",
    "    class_prior[c] = class_p[c]/len(ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fb95d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c', 'j']\n",
      "{'c': 0.75, 'j': 0.25}\n",
      "{'c': {'Chinese': 3, 'Beijing': 1, 'Shanghai': 1, 'Macao': 1, 'Tokyo': 0, 'Japan': 0}, 'j': {'Chinese': 1, 'Beijing': 0, 'Shanghai': 0, 'Macao': 0, 'Tokyo': 1, 'Japan': 1}}\n",
      "{'c': {'Chinese': 0.8, 'Beijing': 0.4, 'Shanghai': 0.4, 'Macao': 0.4, 'Tokyo': 0.2, 'Japan': 0.2}, 'j': {'Chinese': 0.6666666666666666, 'Beijing': 0.3333333333333333, 'Shanghai': 0.3333333333333333, 'Macao': 0.3333333333333333, 'Tokyo': 0.6666666666666666, 'Japan': 0.6666666666666666}}\n"
     ]
    }
   ],
   "source": [
    "print(classes)\n",
    "print(class_prior)\n",
    "print(word_count)\n",
    "print(word_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64c98fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test=[]\n",
    "for i in range(1,3):\n",
    "    with open(\"T\"+str(i)+\".txt\",'r') as mf:\n",
    "        s=mf.read()\n",
    "        s=word_tokenize(s)\n",
    "        s=[w for w in s if w.casefold() not in stop_words]\n",
    "        ll=[]\n",
    "        for a in s:\n",
    "            if a==\"'\" or a==\".\" or a==',' or a==\":\":\n",
    "                continue\n",
    "            else:\n",
    "                ll.append(a)\n",
    "        test.append(ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bcf3e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Chinese', 'Chinese', 'Chinese', 'Tokyo', 'Japan'], ['Chinese', 'Macao']]\n"
     ]
    }
   ],
   "source": [
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04d29a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test)):\n",
    "    a=\"\"\n",
    "    for j in test[i]:        \n",
    "        a+=j+\" \"\n",
    "    test[i]=a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6437b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Chinese Chinese Chinese Tokyo Japan ', 'Chinese Macao ']\n"
     ]
    }
   ],
   "source": [
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "458c22a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_t=[]\n",
    "for i in range(len(test)):\n",
    "    words=test[i].split()\n",
    "    unt=[]\n",
    "    for w in words:\n",
    "        if w not in unique_t:\n",
    "            unt.append(w)\n",
    "    unique_t.append(unt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1085654b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Chinese', 'Chinese', 'Chinese', 'Tokyo', 'Japan'], ['Chinese', 'Macao']]\n"
     ]
    }
   ],
   "source": [
    "print(unique_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb2fb70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lit=[]\n",
    "for i in range(len(unique_t)):\n",
    "    l=[]\n",
    "    for x in unique:\n",
    "        if x in unique_t[i]:\n",
    "            l.append(1)\n",
    "        else:\n",
    "            l.append(0)\n",
    "    lit.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41067462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 0, 0, 0, 1, 1], [1, 0, 0, 1, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "print(lit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "20b86bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Chinese', 'Beijing', 'Shanghai', 'Macao', 'Tokyo', 'Japan']\n"
     ]
    }
   ],
   "source": [
    "print(unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1538ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'c': {'Chinese': 0.8, 'Beijing': 0.4, 'Shanghai': 0.4, 'Macao': 0.4, 'Tokyo': 0.2, 'Japan': 0.2}, 'j': {'Chinese': 0.6666666666666666, 'Beijing': 0.3333333333333333, 'Shanghai': 0.3333333333333333, 'Macao': 0.3333333333333333, 'Tokyo': 0.6666666666666666, 'Japan': 0.6666666666666666}}\n"
     ]
    }
   ],
   "source": [
    "print(word_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8cfbcb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=[]\n",
    "pp=[]\n",
    "\n",
    "for x in range(len(lit)):\n",
    "    ap={}\n",
    "    for c in classes:\n",
    "        p=class_prior[c]\n",
    "        for j in range(len(lit[x])):\n",
    "            if lit[x][j]==1:\n",
    "                p*=word_prob[c][unique[j]]\n",
    "            if lit[x][j]==0:\n",
    "                p*=(1-word_prob[c][unique[j]])           \n",
    "            ap[c]=p\n",
    "    pp.append(ap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3dd7a47d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'c': 0.005184000000000001, 'j': 0.021947873799725653}, {'c': 0.05529600000000001, 'j': 0.0027434842249657075}]\n",
      "['j', 'c']\n"
     ]
    }
   ],
   "source": [
    "print(pp)\n",
    "pred=[]\n",
    "for d in pp:\n",
    "    max_key = max(d, key=d.get)\n",
    "    pred.append(max_key)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3771096f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=['j','j']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3724bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFUSION MATRIX:\n",
      "[[0 0]\n",
      " [1 1]]\n",
      "PRECISION:\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "cmat=np.array(confusion_matrix(y,pred))\n",
    "print(\"CONFUSION MATRIX:\")\n",
    "print(cmat)\n",
    "\n",
    "print(\"PRECISION:\")\n",
    "print(np.trace(cmat)/np.sum(cmat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ef72c77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred=[]\n",
    "# pp={}\n",
    "# for c in classes:\n",
    "#     max_prob = float('-inf')\n",
    "#     ar_max=None\n",
    "#     p=class_prior[c]\n",
    "#     for x in range(len(lit)): \n",
    "#         if lit[x]==1:\n",
    "#             p*=word_prob[c][unique[x]]\n",
    "#         else:\n",
    "#             p*=(1-word_prob[c][unique[x]])\n",
    "#     if p > max_prob:\n",
    "#         max_prob = p\n",
    "#         arg_max = c\n",
    "#     pp[c]=p\n",
    "# pred.append(arg_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "566585e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pp={}\n",
    "# for x in unique_t:\n",
    "#     max_prob = float('-inf')\n",
    "#     ar_max=None\n",
    "#     for c in classes:\n",
    "#         p=class_prior[c]\n",
    "#         for j in unique:\n",
    "#             if j in x:\n",
    "#                 p*=word_prob[c][j]\n",
    "#             else:\n",
    "#                 p*=(1-word_prob[c][j])\n",
    "#         pp[c]=p\n",
    "                \n",
    "            \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
