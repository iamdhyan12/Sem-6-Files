{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afb4de85",
   "metadata": {},
   "source": [
    " <h1>20BCE020 <br>\n",
    "Practical-6 IRS <br>\n",
    "    Implement a program for document classification using Naive Bayes and Bernoulli. Compare the performance of both algorithms using suitable accuracy parameters. <br>\n",
    "Navie Bayes </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78b919ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# nltk.download(\"all\")\n",
    "from nltk.stem import PorterStemmer\n",
    "stm=PorterStemmer()\n",
    "n=4\n",
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c3df4c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Chinese', 'Beijing', 'Chinese'], ['Chinese', 'Chinese', 'Shanghai'], ['Chinese', 'Macao'], ['Tokyo', 'Japan', 'Chinese']]\n"
     ]
    }
   ],
   "source": [
    "fl=[]\n",
    "for i in range(1,n+1):\n",
    "    with open(\"N\"+str(i)+\".txt\",'r') as mf:\n",
    "        s=mf.read()\n",
    "        s=word_tokenize(s)\n",
    "        s=[w for w in s if w.casefold() not in stop_words]\n",
    "        ll=[]\n",
    "        for a in s:\n",
    "            if a==\"'\" or a==\".\" or a==',' or a==\":\":\n",
    "                continue\n",
    "            else:\n",
    "                ll.append(a)\n",
    "        fl.append(ll)\n",
    "print(fl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce76285f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(fl)):\n",
    "    a=\"\"\n",
    "    for j in fl[i]:        \n",
    "        a+=j+\" \"\n",
    "    fl[i]=a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9037a7eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Chinese Beijing Chinese ', 'Chinese Chinese Shanghai ', 'Chinese Macao ', 'Tokyo Japan Chinese ']\n"
     ]
    }
   ],
   "source": [
    "print(fl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb01cf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "ll=['c','c','c','j']\n",
    "unique=[]\n",
    "classes=[]\n",
    "class_prior={}\n",
    "word_count={}\n",
    "total_word_count={}\n",
    "word_prob={}\n",
    "for i in range(len(ll)):\n",
    "    if ll[i] not in classes:\n",
    "        classes.append(ll[i])\n",
    "        class_prior[ll[i]]=1\n",
    "        word_count[ll[i]]={}\n",
    "        total_word_count[ll[i]]=0\n",
    "    else:\n",
    "        class_prior[ll[i]] += 1\n",
    "        \n",
    "for i in range(len(fl)):\n",
    "    words=fl[i].split()\n",
    "    for w in words:\n",
    "        if w not in unique:\n",
    "            unique.append(w)\n",
    "        \n",
    "for i in range(len(fl)):\n",
    "    words=fl[i].split()\n",
    "    for w in words:\n",
    "        if w not in word_count[ll[i]]:\n",
    "            word_count[ll[i]][w]=1\n",
    "        else:\n",
    "            word_count[ll[i]][w] += 1\n",
    "        total_word_count[ll[i]] += 1\n",
    "\n",
    "for c in classes:\n",
    "    word_prob[c]={}\n",
    "    for w in unique:\n",
    "        if w in word_count[c]:\n",
    "            word_prob[c][w]=(word_count[c][w]+1)/(total_word_count[c]+ len(unique))\n",
    "        else:\n",
    "            word_prob[c][w]=(1)/(total_word_count[c]+ len(unique))\n",
    "total_docs=len(ll)\n",
    "for c in classes:\n",
    "    class_prior[c] /= total_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7217bdd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'c': {'Chinese': 0.42857142857142855, 'Beijing': 0.14285714285714285, 'Shanghai': 0.14285714285714285, 'Macao': 0.14285714285714285, 'Tokyo': 0.07142857142857142, 'Japan': 0.07142857142857142}, 'j': {'Chinese': 0.2222222222222222, 'Beijing': 0.1111111111111111, 'Shanghai': 0.1111111111111111, 'Macao': 0.1111111111111111, 'Tokyo': 0.2222222222222222, 'Japan': 0.2222222222222222}}\n"
     ]
    }
   ],
   "source": [
    "print(word_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2844941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Chinese', 'Beijing', 'Shanghai', 'Macao', 'Tokyo', 'Japan']\n"
     ]
    }
   ],
   "source": [
    "print(unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "edaa0bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c', 'j']\n",
      "{'c': 0.75, 'j': 0.25}\n",
      "{'c': {'Chinese': 5, 'Beijing': 1, 'Shanghai': 1, 'Macao': 1}, 'j': {'Tokyo': 1, 'Japan': 1, 'Chinese': 1}}\n",
      "{'c': 8, 'j': 3}\n",
      "{'c': {'Chinese': 0.42857142857142855, 'Beijing': 0.14285714285714285, 'Shanghai': 0.14285714285714285, 'Macao': 0.14285714285714285, 'Tokyo': 0.07142857142857142, 'Japan': 0.07142857142857142}, 'j': {'Chinese': 0.2222222222222222, 'Beijing': 0.1111111111111111, 'Shanghai': 0.1111111111111111, 'Macao': 0.1111111111111111, 'Tokyo': 0.2222222222222222, 'Japan': 0.2222222222222222}}\n"
     ]
    }
   ],
   "source": [
    "print(classes)\n",
    "print(class_prior)\n",
    "print(word_count)\n",
    "print(total_word_count)\n",
    "print(word_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "db3af311",
   "metadata": {},
   "outputs": [],
   "source": [
    "test=[]\n",
    "for i in range(1,3):\n",
    "    with open(\"T\"+str(i)+\".txt\",'r') as mf:\n",
    "        s=mf.read()\n",
    "        s=word_tokenize(s)\n",
    "        s=[w for w in s if w.casefold() not in stop_words]\n",
    "        ll=[]\n",
    "        for a in s:\n",
    "            if a==\"'\" or a==\".\" or a==',' or a==\":\":\n",
    "                continue\n",
    "            else:\n",
    "                ll.append(a)\n",
    "        test.append(ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3a150836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Chinese', 'Chinese', 'Chinese', 'Tokyo', 'Japan'], ['Chinese', 'Macao']]\n"
     ]
    }
   ],
   "source": [
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "88637253",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test)):\n",
    "    a=\"\"\n",
    "    for j in test[i]:        \n",
    "        a+=j+\" \"\n",
    "    test[i]=a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dc17dcbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Chinese Chinese Chinese Tokyo Japan ', 'Chinese Macao ']\n"
     ]
    }
   ],
   "source": [
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0409fe8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_count=[]\n",
    "for x in range(len(test)):\n",
    "    tt={}\n",
    "    words=test[x].split()\n",
    "    for w in words:\n",
    "        if w not in tt:\n",
    "            tt[w]=1\n",
    "        else:\n",
    "            tt[w]+=1\n",
    "    test_count.append(tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "54c2bc3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Chinese': 3, 'Tokyo': 1, 'Japan': 1}, {'Chinese': 1, 'Macao': 1}]\n"
     ]
    }
   ],
   "source": [
    "print(test_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f3d0097a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(test_count['Chinese'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ecca8ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "pred=[]\n",
    "pp={}\n",
    "for x in test:\n",
    "    max_prob = float('-inf')\n",
    "    ar_max=None\n",
    "    for c in classes:\n",
    "        p=class_prior[c]\n",
    "        word=x.split()\n",
    "        for w in word:\n",
    "            p*=pow(word_prob[c][w],1)\n",
    "        if p > max_prob:\n",
    "            max_prob = p\n",
    "            arg_max = c\n",
    "        pp[c]=p\n",
    "    pred.append(arg_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2e06d2a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c', 'c']\n"
     ]
    }
   ],
   "source": [
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "63653a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'c': 0.04591836734693877, 'j': 0.006172839506172839}\n"
     ]
    }
   ],
   "source": [
    "print(pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "80cff781",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=['j','c']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c360aa1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFUSION MATRIX:\n",
      "[[1 0]\n",
      " [1 0]]\n",
      "PRECISION:\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "cmat=np.array(confusion_matrix(y,pred))\n",
    "print(\"CONFUSION MATRIX:\")\n",
    "print(cmat)\n",
    "\n",
    "print(\"PRECISION:\")\n",
    "print(np.trace(cmat)/np.sum(cmat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b5c7345f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for the Test Document is Class: c\n"
     ]
    }
   ],
   "source": [
    "Keymax = max(zip(pp.values(), pp.keys()))[1]\n",
    "print(\"Prediction for the Test Document is Class:\",end=\" \")\n",
    "print(Keymax)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
